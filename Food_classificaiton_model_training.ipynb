{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "img_dir = r\"C:\\Users\\Humperdink2\\Documents\\github\\NewHacks2023\\Food Images\\Food Images\"\n",
    "data_dir = r\"C:\\Users\\Humperdink2\\Documents\\github\\NewHacks2023\\Food Ingredients and Recipe Dataset with Image Name Mapping.csv\"\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "img_size = 169\n",
    "output_size = 13500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the dataloader\n",
    "class foodImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, data_dir, transformation = None):\n",
    "        self.img_dir = img_dir\n",
    "        #self.ingr = pd.read_csv(data_dir, header=1,usecols=[5])\n",
    "        self.transform = transformation\n",
    "        self.labels = pd.read_csv(data_dir, header=1,usecols=[1,4])\n",
    "\n",
    "    #Later augment the dataset to give more images per class to increase accuracy\n",
    "\n",
    "    def __len__(self):\n",
    "       return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.labels.iloc[idx,1] + \".jpg\")\n",
    "        image = read_image(img_path)\n",
    "        label = self.labels.iloc[idx, 0]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    # def __getingrs__(self, index):\n",
    "    #     return self.ingr[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Declaration\n",
    "class FoodModel(nn.Module):\n",
    "    def __init__(self,img_size, output_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,55,kernel_size=(9,9), stride=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "        print(\"init1\")\n",
    "        self.conv2 = nn.Conv2d(55,55,kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        print(\"init2\")\n",
    "        self.flat = nn.Flatten()\n",
    "        print(\"init3\")\n",
    "        self.fc3 = nn.Linear(int(55*55*55/4), int(output_size*2))\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(.5)\n",
    "        print(\"init4\")\n",
    "        self.fc4 = nn.Linear(output_size*2, output_size)\n",
    "        print(\"init5\")\n",
    "    def forward(self, image):\n",
    "        #Transforming to Square\n",
    "        image = transforms.Resize(169)\n",
    "        print(\"train1\")\n",
    "        #First layer\n",
    "        image = self.act1(self.conv1(image))\n",
    "        image = self.drop1(image)\n",
    "        print(\"train2\")\n",
    "        #Second layer\n",
    "        image = self.act2(self.conv2(image))\n",
    "        image = self.pool2(image)\n",
    "        print(\"train3\")\n",
    "        #Third layer\n",
    "        image = self.flat(image)\n",
    "        image = self.act3(self.fc3(image))\n",
    "        image = self.drop3(image)\n",
    "        print(\"train4\")\n",
    "        #output\n",
    "        image = self.fc4(image)\n",
    "        return image\n",
    "    \n",
    "    def train(self, epochs, train_dl, loss_fn, opt):\n",
    "        for epoch in range(epochs):\n",
    "            count = 0\n",
    "            for images, labels in train_dl:\n",
    "                print(\"picture: \", count)\n",
    "                preds = self(images)\n",
    "                loss = loss_fn(preds, labels)\n",
    "                #training\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "model = FoodModel(img_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((169,169)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "food = foodImageDataset(img_dir, data_dir, transformation=transform)\n",
    "food_dl = DataLoader(food, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Humperdink2\\anaconda3\\envs\\newhacks\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Humperdink2\\Documents\\github\\NewHacks2023\\Food_classificaiton_model_training.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Humperdink2/Documents/github/NewHacks2023/Food_classificaiton_model_training.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Running Model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Humperdink2/Documents/github/NewHacks2023/Food_classificaiton_model_training.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain(\u001b[39m5\u001b[39;49m,food_dl,loss_fn,opt)\n",
      "\u001b[1;32mc:\\Users\\Humperdink2\\Documents\\github\\NewHacks2023\\Food_classificaiton_model_training.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Humperdink2/Documents/github/NewHacks2023/Food_classificaiton_model_training.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Humperdink2/Documents/github/NewHacks2023/Food_classificaiton_model_training.ipynb#W6sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Humperdink2/Documents/github/NewHacks2023/Food_classificaiton_model_training.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m train_dl:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Humperdink2/Documents/github/NewHacks2023/Food_classificaiton_model_training.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpicture: \u001b[39m\u001b[39m\"\u001b[39m, count)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Humperdink2/Documents/github/NewHacks2023/Food_classificaiton_model_training.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(images)\n",
      "File \u001b[1;32mc:\\Users\\Humperdink2\\anaconda3\\envs\\newhacks\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Humperdink2\\anaconda3\\envs\\newhacks\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Humperdink2\\anaconda3\\envs\\newhacks\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Humperdink2\\anaconda3\\envs\\newhacks\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\Humperdink2\\Documents\\github\\NewHacks2023\\Food_classificaiton_model_training.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Humperdink2/Documents/github/NewHacks2023/Food_classificaiton_model_training.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels\u001b[39m.\u001b[39miloc[idx, \u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Humperdink2/Documents/github/NewHacks2023/Food_classificaiton_model_training.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Humperdink2/Documents/github/NewHacks2023/Food_classificaiton_model_training.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Humperdink2/Documents/github/NewHacks2023/Food_classificaiton_model_training.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32mc:\\Users\\Humperdink2\\anaconda3\\envs\\newhacks\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Humperdink2\\anaconda3\\envs\\newhacks\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\Humperdink2\\anaconda3\\envs\\newhacks\\lib\\site-packages\\torchvision\\transforms\\functional.py:140\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    138\u001b[0m     _log_api_usage_once(to_tensor)\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (F_pil\u001b[39m.\u001b[39m_is_pil_image(pic) \u001b[39mor\u001b[39;00m _is_numpy(pic)):\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(pic)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m _is_numpy(pic) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[0;32m    143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be 2/3 dimensional. Got \u001b[39m\u001b[39m{\u001b[39;00mpic\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m dimensions.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "#Running Model\n",
    "model.train(5,food_dl,loss_fn,opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(img_dir, data_dir, transformation = None):\n",
    "        data = pd.read_csv(data_dir, header=1)\n",
    "        \n",
    "        # Extracting specific columns for labels and ingredients\n",
    "        labels = data.iloc[:, [1,4]]  # Assuming the label is in column 1 and ingredient in column 4\n",
    "        print(labels)\n",
    "        ingr = data.iloc[:, 5]\n",
    "        print(ingr[6957, 1])\n",
    "\n",
    "get(img_dir,data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newhacks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
